<p><a href="/LICENSE"><img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="License" title="" /></a>
<a href="https://www.tensorflow.org/"><img src="https://img.shields.io/badge/Platform-Tensorflow-orange.svg" alt="Platform" title="" /></a>
<a href=""><img src="https://img.shields.io/badge/Python-3.5-green.svg" alt="Python" title="" /></a>
<a href="https://hub.docker.com/r/ceruleanwang/personae/"><img src="https://img.shields.io/badge/Docker-Available-FF69B4.svg" alt="Docker" title="" /></a></p>

<h1>Personae - RL &amp; SL Methods and Envs For Quantitative Trading</h1>

<p>Personae is a repo that implements papers proposed methods in Deep Reinforcement Learning &amp; Supervised Learning and applies them to Financial Market.</p>

<p>Now Personae includes 4 RL &amp; 3 SL implements and a simulate Financial Market supporting Stock and Future. (Short Sale is still implementing)</p>

<p>More RL &amp; SL methods are updating!</p>

<h1>WARNING</h1>

<p>This repo is being reconstructing,</p>

<p>It will start from 2018-08-24 to ~2018-09-01~ a timestamp that I successfully found a job.</p>

<h1>Attentions</h1>

<ul>
<li>The features as inputs are naive.   </li>
<li>Day frequency is clearly not enough.  </li>
<li>It's recommended that you could replace the features here to your own.   </li>
</ul>

<h1>Contents</h1>

<ul>
<li><p><a href="algorithm/RL/DDPG.py">Deep Deterministic Policy Gradient (DDPG)</a> <br />
Implement of DDPG with TensorFlow.</p>

<blockquote>
  <p>arXiv:1509.02971: <a href="https://arxiv.org/abs/1509.02971">Continuous control with deep reinforcement learning</a></p>
</blockquote></li>
<li><p><a href="algorithm/RL/DoubleDQN.py">Double DQN</a> <br />
Implement of Double-DQN with TensorFlow.   </p>

<blockquote>
  <p>arXiv:1509.06461: <a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a></p>
</blockquote></li>
<li><p><a href="algorithm/RL/DuelingDQN.py">Dueling-DQN</a> <br />
Implement of Dueling-DQN with TensorFlow.    </p>

<blockquote>
  <p>arXiv:1511.06581: <a href="https://arxiv.org/abs/1511.06581">Dueling Network Architectures for Deep Reinforcement Learning</a>     </p>
</blockquote></li>
<li><p><a href="algorithm/RL/PolicyGradient.py">Policy Gradient</a> <br />
Implement of Policy Gradient with TensorFlow.</p>

<blockquote>
  <p>NIPS. Vol. 99. 1999: <a href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">Policy gradient methods for reinforcement learning with function approximation</a></p>
</blockquote></li>
<li><p><a href="algorithm/SL/DualAttnRNN.py">DA-RNN (DualAttnRNN)</a> <br />
Implement of arXiv:1704.02971, DA-RNN with TensorFlow.</p>

<blockquote>
  <p>arXiv:1704.02971: <a href="https://arxiv.org/abs/1704.02971">A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction</a></p>
</blockquote></li>
<li><p><a href="algorithm/SL/TreNet.py">TreNet (HNN)</a> <br />
Implement of TreNet with TensorFlow.    </p>

<blockquote>
  <p>IJCAI 2017. <a href="https://www.ijcai.org/proceedings/2017/0316.pdf">Hybrid Neural Networks for Learning the Trend in Time Series</a></p>
</blockquote></li>
<li><p><a href="algorithm/SL/NaiveLSTM.py">Naive-LSTM (LSTM)</a> <br />
Implement of simple LSTM based model with TensorFlow.    </p>

<blockquote>
  <p>arXiv:1506.02078: <a href="https://arxiv.org/abs/1506.02078">Visualizing and Understanding Recurrent Networks</a>     </p>
</blockquote></li>
</ul>

<h1>Environment</h1>

<p>A basic simulate environment of Financial Market is implemented.</p>

<ul>
<li><a href="base/env/market.py">Market</a> <br />
Implement of Market, Trader, Positions as a gym env (gym is not required), which can give a env for regression or sequence data generating for RL or SL model. <br />
For now, Market support Stock Data and Future Data.</li>
</ul>

<p>Also, more functions are updating.</p>

<h1>Experiments</h1>

<ul>
<li><a href="algorithm/SL/DualAttnRNN.py">Deep Deterministic Policy Gradient (DDPG)</a></li>
<li><a href="algorithm/RL/DoubleDQN.py">Double-DQN</a>    </li>
<li><a href="algorithm/RL/DuelingDQN.py">Dueling-DQN</a></li>
<li><a href="algorithm/RL/PolicyGradient.py">Policy Gradient (PG)</a> <br />
Train a Agent to trade in stock market, using stock data set from 2012-01-01 to 2018-01-01 where 70% are training data, 30% are testing data.</li>
</ul>

<p align='center'>
  <img src='static/images/Profits.jpg'>
   <em>Total Profits and Baseline Profits. (Test Set)</em>
</p>

<ul>
<li><a href="algorithm/SL/DualAttnRNN.py">DA-RNN (DualAttnRNN)</a>   </li>
<li><a href="algorithm/SL/NaiveLSTM.py">Naive-LSTM (LSTM)</a>      </li>
<li><a href="algorithm/SL/TreNet.py">TreNet (HNN)</a> <br />
Train a Predictor to predict stock prices, using stock data set from 2008-01-01 to 2018-01-01, where 70% are training data, 30% are testing data.</li>
</ul>

<p align='center'>
  <img src='static/images/Prices.jpg'>
   <em>Prices Prediction Experiments on 4 Bank Stocks. (Test Set)</em>
</p>

<h1>Requirements</h1>

<p>Before you start testing, following requirements are needed.</p>

<ul>
<li>Python3.5</li>
<li>TensorFlow1.4</li>
<li>numpy</li>
<li>scipy</li>
<li>pandas</li>
<li>rqalpha</li>
<li>sklearn</li>
<li>tushare</li>
<li>matplotlib</li>
<li>mongoengine</li>
<li>CUDA (option)</li>
<li>ta-lib (option)</li>
<li>Docker (option)</li>
<li>PyTorch (option)</li>
</ul>

<p>It's best that if you are a Docker user, so that you can run the whole project without installing those dependencies manually.</p>

<p>And you can also use <a href="http://www.ansible.com.cn/">Ansible</a> to run <a href="playbook/cuda-playbook.yml">CUDA-Playbook</a> and <a href="playbook/docker-playbook.yml">Docker-Playbook</a> to install CUDA and <a href="https://github.com/NVIDIA/nvidia-docker">Nvidia-Docker</a> if you want to run tests in a Docker Container.</p>

<h1>How to Use</h1>

<h3>If you use Docker</h3>

<h4>About base image</h4>

<p>My image for this repo is <a href="https://hub.docker.com/r/ceruleanwang/personae/">ceruleanwang/personae</a>, and personae is inherited from <a href="https://hub.docker.com/r/ceruleanwang/quant-base/">ceruleanwang/quant-base</a>.    </p>

<p>The image <a href="https://hub.docker.com/r/ceruleanwang/quant-base/">ceruleanwang/quant-base</a> is inherited from <a href="https://hub.docker.com/r/nvidia/cuda/">nvidia/cuda:8.0-cudnn6-runtime</a>. So please make sure your CUDA version and cuDNN version are correct. </p>

<h4>Instructions</h4>

<p>First you should make sure you have stocks data in your mongodb.   </p>

<p>If you don't have, you can use a spider writen in this repo to crawl stock or future data, but before you start, you should make sure a mongodb service is running.  </p>

<p>If you don't have mongodb service running, you can also use a mongodb container (option) by following code:
<code>
docker run -p 27017:27017 -v /data/db:/data/db -d --network=your_network mongo
</code> 
Then, you can use spiders to crawl stock data by following code: <br />
<code>
docker run -t -v local_project_dir:docker_project_dir --network=your_network ceruleanwang/personae spider/stock_spider.py
</code>
Also, you can crawl future data by following code:
<code>
docker run -t -v local_project_dir:docker_project_dir --network=your_network ceruleanwang/personae spider/future_spider.py
</code>
But remember to set stock or future codes that you want to crawl, the default stock codes are:
<code>
stock_codes = ["600036", "601328", "601998", "601398"]
</code>
And the default future codes are:
<code>
future_codes = ["AU88", "RB88", "CU88", "AL88"]
</code>
And they can be modified in: <br />
+ <a href="helper/args_parser.py">Default Args Parser</a></p>

<p>Then you can just run a model by:
<code>
docker run -t -v local_project_dir:docker_project_dir --network=yuor_network ceruleanwang/personae algorithm/RL or SL/algorithm_name.py
</code></p>

<h3>If you use Conda</h3>

<p>You can create an env yourself, and install Python3.5 and all dependencies required, then just run algorithm in your way.   </p>

<p>One thing should be noticed is that the hostname in <a href="/base/model/__init__.py">mongoengine</a> config should be your own.</p>

<h3>About training &amp; testing</h3>

<p>For now, all models implemented with TensorFlow support persistence. You can edit many parameters when you are training or testing a model. <br />
For example, following codes show some parameters that could be edited.
```
env = Market(codes, start<em>date="2008-01-01", end</em>date="2018-01-01", **{
    "market": market,
    "mix<em>index</em>state": True,
    "training<em>data</em>ratio": training<em>data</em>ratio,
})</p>

<p>algorithm = Algorithm(tf.Session(config=config), env, env.trader.action<em>space, env.data</em>dim, **{
    "mode": mode,
    "episodes": episode,
    "enable<em>saver": True,
    "enable</em>summary<em>writer": True,
    "save</em>path": os.path.join(CHECKPOINTS<em>DIR, "RL", model</em>name, market, "model"),
    "summary<em>path": os.path.join(CHECKPOINTS</em>DIR, "RL", model_name, market, "summary"),
})
```</p>

<h1>TODO</h1>

<ul>
<li>More Implementations of Papers.</li>
<li>More High-Frequency Stocks Data.</li>
</ul>
