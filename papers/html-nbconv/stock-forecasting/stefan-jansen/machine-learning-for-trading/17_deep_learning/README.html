<h1>Deep Learning for Trading</h1>

<p>This chapter kicks off part four, which covers how several deep learning (DL) modeling techniques can be useful for investment and trading. DL has achieved numerous breakthroughs in many domains ranging from image and speech recognition to robotics and intelligent agents that have drawn widespread attention and revived large-scale research into Artificial Intelligence (AI). The expectations are high that the rapid development will continue and many more solutions to difficult practical problems will emerge.</p>

<p>In this chapter, we will present feedforward neural networks to introduce key elements of working with neural networks relevant to the various DL architectures covered in the following chapters. More specifically, we will demonstrate how to train large models efficiently using the backpropagation algorithm and manage the risks of overfitting. We will also show how to use the popular Keras, TensorFlow 2.0, and PyTorch frameworks, which we will leverage throughout part four.</p>

<p>In the following chapters, we will build on this foundation to design various architectures suitable for different investment applications with a particular focus on alternative text and image data. These include recurrent neural networks (RNNs) tailored to sequential data such as time series or natural language, and Convolutional Neural Networks (CNNs), which are particularly well suited to image data but can also be used with time-series data. We will also cover deep unsupervised learning, including autoencoders and Generative Adversarial Networks (GANs) as well as reinforcement learning to train agents that interactively learn from their environment.</p>

<h2>Content</h2>

<ol>
<li><a href="#deep-learning-how-it-differs-and-why-it-matters">Deep learning: How it differs and why it matters</a>
<ul>
<li><a href="#how-hierarchical-features-help-tame-high-dimensional-data">How hierarchical features help tame high-dimensional data</a></li>
<li><a href="#automating-feature-extraction-dl-as-representation-learning">Automating feature extraction: DL as representation learning</a></li>
<li><a href="#how-dl-relates-to-machine-learning-and-artificial-intelligence">How DL relates to machine learning and artificial intelligence</a></li>
</ul></li>
<li><a href="#code-example-designing-a-neural-network">Code example: Designing a neural network</a>
<ul>
<li><a href="#key-design-choices">Key design choices</a></li>
<li><a href="#how-to-regularize-deep-neural-networks">How to regularize deep neural networks</a></li>
<li><a href="#training-faster-optimizations-for-deep-learning">Training faster: Optimizations for deep learning</a></li>
</ul></li>
<li><a href="#popular-deep-learning-libraries">Popular Deep Learning libraries</a>
<ul>
<li><a href="#how-to-leverage-gpu-optimization">How to Leverage GPU Optimization</a></li>
<li><a href="#how-to-use-tensorboard">How to use Tensorboard</a></li>
<li><a href="#code-example-how-to-use-pytorch">Code example: how to use PyTorch</a></li>
<li><a href="#code-example-how-to-use-tensorflow">Code example: How to use TensorFlow</a></li>
</ul></li>
<li><a href="#code-example-optimizing-a-neural-network-for-a-long-short-trading-strategy">Code example: Optimizing a neural network for a long-short trading strategy</a>
<ul>
<li><a href="#optimizing-the-nn-architecture">Optimizing the NN architecture</a></li>
<li><a href="#backtesting-a-long-short-strategy-based-on-ensembled-signals">Backtesting a long-short strategy based on ensembled signals</a></li>
</ul></li>
</ol>

<h2>Deep learning: How it differs and why it matters</h2>

<p>The machine learning (ML) algorithms covered in Part 2 work well on a wide variety of important problems, including on text data as demonstrated in Part 3. They have been less successful, however, in solving central AI problems such as recognizing speech or classifying objects in images. These limitations have motivated the development of DL, and the recent DL breakthroughs have greatly contributed to a resurgence of interest in AI. F</p>

<p>or a comprehensive introduction that includes and expands on many of the points in this section, see Goodfellow, Bengio, and Courville (2016), or for a much shorter version, see LeCun, Bengio, and Hinton (2015).</p>

<ul>
<li><a href="https://www.deeplearningbook.org/">Deep Learning</a>, Ian Goodfellow, Yoshua Bengio and Aaron Courville, MIT Press, 2016</li>
<li><a href="https://www.nature.com/articles/nature14539">Deep learning</a>, Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, Nature 2015</li>
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a>, Michael A. Nielsen, Determination Press, 2015</li>
<li><a href="https://ai.stanford.edu/~nilsson/QAI/qai.pdf">The Quest for Artificial Intelligence - A History of Ideas and Achievements</a>, Nils J. Nilsson, Cambridge University Press, 2010</li>
<li><a href="https://ai100.stanford.edu/">One Hundred Year Study on Artificial Intelligence (AI100)</a></li>
<li><a href="http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.71056&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">TensorFlow Playground</a>, Interactive, browser-based Deep Learning platform</li>
</ul>

<h3>How hierarchical features help tame high-dimensional data</h3>

<p>As discussed throughout Part 2, the key challenge of supervised learning is to generalize from training data to new samples. Generalization becomes exponentially more difficult as the dimensionality of the data increases. We encountered the root causes of these difficulties as the curse of dimensionality in Chapter 13, <a href="../13_unsupervised_learning">Unsupervised Learning: From Data-Driven Risk Factors to Hierarchical Risk Parity</a>.</p>

<h3>Automating feature extraction: DL as representation learning</h3>

<p>Many AI tasks like image or speech recognition require knowledge about the world. One of the key challenges is to encode this knowledge so a computer can utilize it. For decades, the development of ML systems required considerable domain expertise to transform the raw data (such as image pixels) into an internal representation that a learning algorithm could use to detect or classify patterns.</p>

<h3>How DL relates to machine learning and artificial intelligence</h3>

<p>AI has a long history, going back at least to the 1950s as an academic field and much longer as a subject of human inquiry, but has experienced several waves of ebbing and flowing enthusiasm since (see <a href="https://ai.stanford.edu/~nilsson/QAI/qai.pdf">The Quest for Artificial Intelligence</a>, Nilsson, 2009 for an in-depth survey). 
- ML is an important subfield with a long history in related disciplines such as statistics and became prominent in the 1980s. 
- DL is a form of representation learning and itself a subfield of ML.</p>

<h2>Code example: Designing a neural network</h2>

<p>To gain a better understanding of how NN work, the notebook <a href="build_and_train_feedforward_nn.ipynb">01<em>build</em>and<em>train</em>feedforward_nn</a> formulates as simple feedforward architecture and forward propagation computations using matrix algebra and implements it using Numpy, the Python counterpart of linear algebra.</p>

<p align="center">
<img src="https://i.imgur.com/UKCr9zi.png" width="85%">
</p>

<h3>Key design choices</h3>

<p>Some NN design choices resemble those for other supervised learning models. For example, the output is dictated by the type of the ML problem such as regression, classification, or ranking. Given the output, we need to select a cost function to measure prediction success and failure, and an algorithm that optimizes the network parameters to minimize the cost. </p>

<p>NN-specific choices include the numbers of layers and nodes per layer, the connections between nodes of different layers, and the type of activation functions.</p>

<h3>How to regularize deep neural networks</h3>

<p>The downside of the capacity of NN to approximate arbitrary functions is the greatly increased risk of overfitting. The best protection against overfitting is to train the model on a larger dataset. Data augmentation, e.g. by creating slightly modified versions of images, is a powerful alternative approach. The generation of synthetic financial training data for this purpose is an active research area that we will address in <a href="../21_gans_for_synthetic_time_series">Chapter 21</a></p>

<h3>Training faster: Optimizations for deep learning</h3>

<p>Backprop refers to the computation of the gradient of the cost function with respect to the internal parameter we wish to update and the use of this information to update the parameter values. The gradient is useful because it indicates the direction of parameter change that causes the maximal increase in the cost function. Hence, adjusting the parameters according to the negative gradient produces an optimal cost reduction, at least for a region very close to the observed samples. See Ruder (2016) for an excellent overview of key gradient descent optimization algorithms.</p>

<ul>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization">Gradient Checking &amp; Advanced Optimization</a>, Unsupervised Feature Learning and Deep Learning, Stanford University</li>
<li><a href="http://ruder.io/optimizing-gradient-descent/index.html#momentum">An overview of gradient descent optimization algorithms</a>, Sebastian Ruder, 2016</li>
</ul>

<h2>Popular Deep Learning libraries</h2>

<p>Currently, the most popular DL libraries are <a href="https://www.tensorflow.org/">TensorFlow</a> (supported by Google) and <a href="https://pytorch.org/">PyTorch</a> (supported by Facebook). </p>

<p>Development is very active with PyTorch at version 1.4 and TensorFlow at 2.2 as of March 2020. TensorFlow 2.0 adopted <a href="https://keras.io/">Keras</a> as its main interface, effectively combining both libraries into one.
Additional options include:</p>

<ul>
<li><a href="https://github.com/Microsoft/CNTK">Microsoft Cognitive Toolkit (CNTK)</a></li>
<li><a href="http://caffe.berkeleyvision.org/">Caffe</a></li>
<li><a href="http://www.deeplearning.net/software/theano/">Thenao</a>, developed at University of Montreal since 2007</li>
<li><a href="https://mxnet.apache.org/">Apache MXNet</a>, used by Amazon</li>
<li><a href="https://chainer.org/">Chainer</a>, developed by the Japanese company Preferred Networks</li>
<li><a href="http://torch.ch/">Torch</a>, uses Lua, basis for PyTorch</li>
<li><a href="https://deeplearning4j.org/">Deeplearning4J</a>, uses Java</li>
</ul>

<h3>How to Leverage GPU Optimization</h3>

<p>All popular Deep Learning libraries support the use of GPU, and some also allow for parallel training on multiple GPU. The most common types of GPU are produced by NVIDA, and configuration requires installation and setup of the CUDA environment. The process continues to evolve and can be somewhat challenging depending on your computational environment. </p>

<p>A more straightforward way to leverage GPU is via the the Docker virtualization platform. There are numerous images available that you can run in local container managed by Docker that circumvents many of the driver and version conflicts that you may otherwise encounter. Tensorflow provides docker images on its website that can also be used with Keras. </p>

<ul>
<li><a href="http://timdettmers.com/2018/11/05/which-gpu-for-deep-learning/">Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning</a>, Tim Dettmers</li>
<li><a href="http://timdettmers.com/2018/12/16/deep-learning-hardware-guide/">A Full Hardware Guide to Deep Learning</a>, Tim Dettmers</li>
</ul>

<h3>How to use Tensorboard</h3>

<p>Tensorboard is a great visualization tool that comes with TensorFlow. It includes a suite of visualization tools to simplify the understanding, debugging, and optimization of neural networks.</p>

<p>You can use it to visualize the computational graph, plot various execution and performance metrics, and even visualize image data processed by the network. It also permits comparisons of different training runs.
When you run the how<em>to</em>use_keras notebook, and with TensorFlow installed, you can launch Tensorboard from the command line:</p>

<p><code>python
tensorboard --logdir=/full_path_to_your_logs ## e.g. ./tensorboard
</code>
- <a href="https://www.tensorflow.org/guide/summaries_and_tensorboard">TensorBoard: Visualizing Learning</a></p>

<h3>Code example: how to use PyTorch</h3>

<p>Pytorch has been developed at the Facebook AI Research group led by Yann LeCunn and the first alpha version released in September 2016. It provides deep integration with Python libraries like Numpy that can be used to extend its functionality, strong GPU acceleration, and automatic differentiation using its autograd system. It provides more granular control than Keras through a lower-level API and is mainly used as a deep learning research platform but can also replace NumPy while enabling GPU computation.</p>

<p>It employs eager execution, in contrast to the static computation graphs used by, e.g., Theano or TensorFlow. Rather than initially defining and compiling a network for fast but static execution, it relies on its autograd package for automatic differentiation of Tensor operations, i.e., it computes gradients ‘on the fly’ so that network structures can be partially modified more easily. This is called define-by-run, meaning that backpropagation is defined by how your code runs, which in turn implies that every single iteration can be different. The PyTorch documentation provides a detailed tutorial on this.</p>

<p>The notebook <a href="03_how_to_use_pytorch.ipynb">how<em>to</em>use_pytorch</a> illustrates how to use the 1.4 release.</p>

<ul>
<li><a href="https://pytorch.org/docs">PyTorch Documentation</a></li>
<li><a href="https://pytorch.org/tutorials">PyTorch Tutorials</a></li>
<li><a href="https://pytorch.org/ecosystem">PyTorch Ecosystem</a>
<ul>
<li><a href="https://allennlp.org/">AllenNLP</a>, state-of-the-art NLP platform developed by the Allen Institute for Artificial Intelligence</li>
<li><a href="https://github.com/zalandoresearch/flair">Flair</a>,  simple framework for state-of-the-art NLP developed at Zalando</li>
<li><a href="http://www.fast.ai/">fst.ai</a>, simplifies training NN using modern best practices; offers online training</li>
</ul></li>
</ul>

<h3>Code example: How to use TensorFlow</h3>

<p>TensorFlow has become the leading deep learning library shortly after its release in September 2015, one year before PyTorch. TensorFlow 2.0 aims to simplify the API that has grown increasingly complex over time by making the Keras API, integrated into TensorFlow as part of the contrib package since 2017 its principal interface, and adopting eager execution. It will continue to focus on a robust implementation across numerous platforms but will make it easier to experiment and do research.</p>

<p>The notebook <a href="04_how_to_use_tensorflow.ipynb">how<em>to</em>use_tensorflow</a> illustrates how to use the 2.0 release.</p>

<ul>
<li><a href="https://www.tensorflow.org/">TensorFlow.org</a></li>
<li><a href="https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a">Standardizing on Keras: Guidance on High-level APIs in TensorFlow 2.0</a></li>
<li><a href="https://js.tensorflow.org/">TensorFlow.js</a>, A JavaScript library for training and deploying ML models in the browser and on Node.js</li>
</ul>

<h2>Code example: Optimizing a neural network for a long-short trading strategy</h2>

<p>In practice, we need to explore variations for the design options for the NN architecture and how we train it from those we outlined previously because we can never be sure from the outset which configuration best suits the data. </p>

<p>This code example explores various architectures for a simple feedforward neural network to predict daily stock returns using the dataset developed in <a href="../12_gradient_boosting_machines">Chapter 12</a> (see the notebook <a href="../12_gradient_boosting_machines/04_preparing_the_model_data.ipynb">preparing<em>the</em>model_data</a>).</p>

<p>To this end, we will define a function that returns a TensorFlow model based on several architectural input parameters and cross-validate alternative designs using the MultipleTimeSeriesCV we introduced in Chapter 7. To assess the signal quality of the model predictions, we build a simple ranking-based long-short strategy based on an ensemble of the models that perform best during the in-sample cross-validation period. To limit the risk of false discoveries, we then evaluate the performance of this strategy for an out-of-sample test period.</p>

<h3>Optimizing the NN architecture</h3>

<p>The notebook <a href="04_how_to_use_tensorflow.ipynb">how<em>to</em>optimize<em>a</em>NN_architecure</a> explores various options to build a simple feedforward Neural Network to predict asset returns. To develop our trading strategy, we use the daily stock returns for 995 US stocks for the eight-year period from 2010 to 2017. </p>

<h3>Backtesting a long-short strategy based on ensembled signals</h3>

<p>To translate our NN model into a trading strategy, we generate predictions, evaluate their signal quality, create rules that define how to trade on these predictions, and backtest the performance of a strategy that implements these rules. </p>

<p>The notebook <a href="05_backtesting_with_zipline.ipynb">backtesting<em>with</em>zipline</a> contains the code examples for this section.</p>
